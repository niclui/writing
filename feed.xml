<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://niclui.github.io/writing/feed.xml" rel="self" type="application/atom+xml" /><link href="https://niclui.github.io/writing/" rel="alternate" type="text/html" /><updated>2021-09-13T00:41:35-05:00</updated><id>https://niclui.github.io/writing/feed.xml</id><title type="html">An adventure into AI</title><entry><title type="html">Paper Review - Squeeze &amp;amp; Excitation Networks</title><link href="https://niclui.github.io/writing/paper%20review/channel%20attention/deep%20learning/2021/09/12/Paper-Review-Squeeze-&-Excitation-Networks.html" rel="alternate" type="text/html" title="Paper Review - Squeeze &amp;amp; Excitation Networks" /><published>2021-09-12T00:00:00-05:00</published><updated>2021-09-12T00:00:00-05:00</updated><id>https://niclui.github.io/writing/paper%20review/channel%20attention/deep%20learning/2021/09/12/%5BPaper-Review%5D-Squeeze-&amp;-Excitation-Networks</id><author><name>Nicholas</name></author><category term="paper review" /><category term="channel attention" /><category term="deep learning" /><summary type="html">A review of Hu et al (2017)â€™s paper on squeeze and excitation networks.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://niclui.github.io/writing/images/SEB.png" /><media:content medium="image" url="https://niclui.github.io/writing/images/SEB.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A gentle introduction to gradient descent</title><link href="https://niclui.github.io/writing/gradient%20descent/deep%20learning/2021/09/04/A-gentle-introduction-to-gradient-descent.html" rel="alternate" type="text/html" title="A gentle introduction to gradient descent" /><published>2021-09-04T00:00:00-05:00</published><updated>2021-09-04T00:00:00-05:00</updated><id>https://niclui.github.io/writing/gradient%20descent/deep%20learning/2021/09/04/A-gentle-introduction-to-gradient-descent</id><author><name>Nicholas</name></author><category term="gradient descent" /><category term="deep learning" /><summary type="html">Gradient descent is one of the key foundations of machine learning. Today, I hope to give a high-level and intuitive overview of this concept.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://niclui.github.io/writing/images/skye.png" /><media:content medium="image" url="https://niclui.github.io/writing/images/skye.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building an image classifier with fast.ai</title><link href="https://niclui.github.io/writing/deep%20learning/transfer%20learning/image%20classification/2021/08/29/Building-an-image-classifier-with-fast.ai.html" rel="alternate" type="text/html" title="Building an image classifier with fast.ai" /><published>2021-08-29T00:00:00-05:00</published><updated>2021-08-29T00:00:00-05:00</updated><id>https://niclui.github.io/writing/deep%20learning/transfer%20learning/image%20classification/2021/08/29/Building-an-image-classifier-with-fast.ai</id><author><name></name></author><category term="deep learning" /><category term="transfer learning" /><category term="image classification" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://niclui.github.io/writing/images/dogcatpic.jpg" /><media:content medium="image" url="https://niclui.github.io/writing/images/dogcatpic.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>