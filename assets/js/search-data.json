{
  
    
        "post0": {
            "title": "Semantic segmentation of images of unmanned spacecraft",
            "content": "I would like to express my deep gratitude to my wonderful teammates (William Armstrong and Spencer Drakontaidis) for their consistently outstanding work, strong collaborative spirit, and willingness to tackle difficult, open-ended problems head on. In all sincerity, I could not have asked for better people to work with. Here is our project report and github. . Motivation . The goal of semantic segmentation is to label different parts of an image with a corresponding class. We have achieved remarkable results in many domains, such as medical images. . However, there has been little progress made on segmenting images of satellites (i.e. developing a model that can identify different satellite parts, such as the solar panels and antenna, from a given image). This is not ideal. Developing a system that can do so is fundamental to many important applications such as autonomous satellite rendezvous (i.e. enabling satellites to rendezvous and dock by another unmanned spacecraft with zero/little human input). . The key obstacle is the lack of a labelled dataset of satellites. Pictures of satellites in space are hard to come by (much less labelled ones). To address this, we generated a prototype synthetic dataset of labelled satellites and trained a variety of state-of-the-art segmentation models on it for benchmark results. . Dataset . We use NASA’s open-source 3D models of satellites to produce our synthetic dataset. To provide our dataset with a variety of spacecraft configurations, we chose the Chandra X-Ray Observatory, Near Earth Asteroid Rendezvous – Shoemaker (NEAR Shoemaker), Cluster II, and the IBEX Interstellar Boundary Explorer, as 3D models from which to generate synthetic images. We used the Blender software to process the 3D models. . Step 1: Labelling 3D models . In consultation with an industry expert (Kevin Okseniuk, systems test engineer at Momentus), we identified eleven classes for our segmentation task. The classes include solar panels, antennas, and thrusters. These classes were chosen because they are crucial to automous satellite rendezvous. They include satellite parts that we should avoid during rendezvous (e.g. thrusters which produce liquid that may obstruct the view of the docking spacecraft) and parts that we should fixate on (e.g. the bottom ring of the satellite which provides a good grip point for the docking spacecraft). . Using Blender, we then labelled each part of the satellite with a unique color. . Step 2: Artistic modifications . We compose a series of artistic modifications to make the 3D models look more realistic. For instance, we simulated the lighting conditions of low earth orbit by illuminating the model with two light sources: one light source at infinity simulating the intensity, color, and parallel light rays of the sun, and one planar light source to simulate earthshine, i.e. the sunlight reflected by the surface of the earth. . Step 3: Generating synthetic dataset . We wrote a Python script to move the camera in Blender in a spherical pattern around the 3D model to one of 5000 positions. For each position, three rendered images were generated with the same aspect, but with different ranges. This gave us 15,000 images for each 3D model and a total of 60,000 images. . This process was repeated for both the unlabelled and labelled 3D models, giving us 60,000 base image and ground truth pairs. . Step 4: Ground truth representation . Originally, for a given synthetic image, each pixel has three values for its R/G/B colors. To make these images more understandable for our model, we used Python’s Pillow library to map each combination of RGB values to the corresponding class label (ranging from 0 to 10). Subsequently, each pixel of a synthetic image contains only one value (from 0 to 10) which corresponds to its respective class. . Model Training . Architectures . After preparing our synthetic dataset, we proceed with training 3 state-of-the-art deep learning segmentation models using Python’s FastAI and SemTorch libraries. In each case, a backbone pre-trained on ImageNet was incorporated to leverage transfer learning in extracting features from the input image. . I. U-Net . II. HRNet . III. DeepLab . Loss Functions . We also experimented with a variety of loss functions to mitigate the class imbalance in the dataset (for instance, the background/non-essential satellite parts takes up ~94% of all pixels). . I. Categorical Cross-Entropy Loss . For each pixel, this function computes the log loss summed over all possible classes. . $Loss_i = - sum_{classes} y log( hat{y})$ . This scoring is computed over all pixels and the average taken. However, this loss function is susceptible to class imbalance. For unbalanced data, training might be dominated by the most prevalent class. . II. Dice Score . For a given pixel, we compute the F1 score (also known as the Dice Coefficient) for all 11 classes. Then, calculate the arithmetic mean. The Dice Score is given by 1 minus the mean. We are able to mitigate class imbalance as the F1 score balances between precision and recall. . III. Dice Score + Focal Loss . Focal loss modifies the pixel-wise cross-entropy loss by down-weighting the loss of easy-to-classify pixels based on a hyperparamter $ gamma$, focusing training on more difficult examples. The focal loss is given by: . $FocalLoss_i = - sum_{classes} (1 - hat{y})^{ gamma} y log( hat{y})$ . Dice + focal loss blends Dice and focal loss with a mixing parameter α applied to the focal loss, balancing global (Dice) and local (focal) features of the target mask. We used the default values of $ gamma$ = 2 and $ alpha$ = 1 during training . Results . Conclusion .",
            "url": "https://niclui.github.io/writing/projects/semantic%20segmentation/deep%20learning/2021/12/31/Semantic-segmentation-of-images-of-satellites.html",
            "relUrl": "/projects/semantic%20segmentation/deep%20learning/2021/12/31/Semantic-segmentation-of-images-of-satellites.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A primer on multi-task learning",
            "content": "Hi everyone! Today I would like to share about a new topic in ML that I have been exploring: multi-task learning (MTL). In MTL, we train a “generalist” model that is able to perform a variety of tasks. By exploiting similarities between the different tasks, we are able to achieve better performance more quickly for each individual task. Curious to see how this works? Let’s dive in! . Overview . Let’s say that we need to perform 3 different tasks - detecting a dog, detecting a cat, and detecting a giraffe. The standard approach would be to train individual CNNs for each task. However, we may run into data limitations. Neural networks perform very well on large and diverse datasets. However, our dataset for each task may be small, leading to poor performance for each individual task. Intuitively, if our model has only a few dog pictures to learn from, it may not be able to differentiate between important dog-specific features and irrelevant features. This may lead to us over-fitting on irrelevant features, hurting the model’s generalizability. . Multi-task learning is able to circumvent these limitations. Instead of having one model for each task, we build a “generalist” model that performs all three tasks simultaneously. This is because the different tasks share similarities. For instance, both dog and cat detection will require the model to identify the presence of fur. By exploiting similarities between the different tasks, we are able to achieve good performance even with limited datasets for each task. Let us first go through the technical details of how MTL exploits task similarities through parameter sharing. Afterwards, we will give an intuitive illustration of how this improves performance. . Parameter Sharing . There are two types of parameter sharing - hard parameter sharing (where every task shares a large number of common parameters) and soft parameter sharing (where we constrain the parameters of different tasks, ensuring that they are similar to each other). This encourages the model to learn a general representation across the different tasks, reducing the risk of us overfitting on any one task. . Hard Parameter Sharing . We will explore the most basic ways to introduce hard parameter sharing, which is the most common approach in MTL. . Method 1: Multi-head architecture . One simple way is to design an architecture where the hidden layers are shared among all the tasks. Only the output layer is task-specific and we have multiple heads corresponding to each task. . . Source: Ruder (2017) Method 2: Conditioning on task descriptor vector . We can create a task descriptor vector, which contains information about the different tasks. For instance, we can create a one-hot vector of the task index. Task 1 will correspond to [1,0,0,…], task 2 will correspond to [0,1,0,…] and so on. By conditioning on this vector, we are able to control where and how we split the set of parameters into common parameters and task-specific ones. . The simplest form of conditioning is concatenation-based. Consider the following CNN. In this set-up, the convolutional layers are shared among all the tasks. However, at the first fully connected layer, we can concatenate the input vector with the task index vector (denoted as z). . . Source: Stanford CS330 Course In other words, we stack x on top of z. Correspondingly, we expand the weights matrix of the FC layer. . . Source: Stanford CS330 Course This weights matrix will now contain both common parameters and task-specific ones. Parameters in columns 1 to D are shared across all tasks. In contrast, parameters in columns D+1 to T are task-specific. To see this, consider parameters in column D+1. These parameters will be multiplied with z1 in matrix multiplication. If z1 is one (i.e. task 1), these parameters will be “activated”. If z1 is zero (i.e. any task other than task 1), these parameters will be multiplied with a zero value. As such, parameters in column D+1 are specific to task 1. Column D+2 is specific to task 2, and so on and so forth. . . Source: Personal drawing Overall, we have a model that shares the hidden layers across the tasks. In the fully-connected layers, there is a mix of shared parameters and task-specific ones. By varying the location where we introduce the task index vector, we are able to control where the split of parameters happen. . There are other options such as multiplicative-based conditioning. Instead of stacking x on top of z, we can use the product of x and z. This allows us to capture interaction effects - the weight applied to x differs based on the value of z. The choice of z is another important consideration. Instead of using a one-hot vector of the task index, we can consider choosing a descriptor vector that contains metadata of the different tasks. This will allow us to better exploit similarities between the different task structures. . Soft Parameter Sharing . In soft parameter sharing, each task has its own model with its own parameters. However, to encourage cross-learning between the different tasks, we impose regularization. The regularization penalty (e.g. L2 norm between the parameter vectors of different models) forces the parameters of the different tasks to be close to each other, ensuring that the model learns a more general representation across the different tasks. . . Source: Ruder (2017) Hard vs Soft Parameter Sharing . So which type of parameter sharing is better? It depends. Hard parameter sharing greatly reduces the risk of overfitting on a specific task. Intuitively, given that the different tasks share a significant % of the set of parameters, our model is more likely to find a representation that generalizes across all the tasks. . The downside of hard parameter sharing is its lack of flexibility. We impose a large number of common parameters for the different tasks. This is potentially problematic - let’s say that we have a feature (e.g. presence of a snout) that is more useful for dog detection than giraffe detection. For the best results, we want to attach a heavier weight to this feature for dog detection (and a smaller weight for giraffe detection). However, in hard parameter sharing, we impose the same weight for all classes, affecting the model’s performance. . Soft parameter sharing provides a compromise between this trade-off. It is slightly more prone to overfitting, but allows for greater flexibility in parameter choice. . Why MTL works . MTL tries to exploit the similarities between different tasks. There are a few mechanisms through which this exploitation can improve model performance and reduce training time: . (I) Implicit data augmentation: MTL effectively allows us to expand our dataset for any given task. All tasks are somewhat noisy. For instance, the dataset for any given task is never perfect - it may contain a few wrong labels or low quality images. In MTL, we train on multiple tasks. Assuming that the noise patterns of different tasks are unrelated, we are able to average the noise patterns across a variety of tasks. . (II) Attention focusing: If we have an extremely noisy task (e.g. a large percentage of pictures are blurry), there is a high risk of overfitting on less important features. MTL can help the model focus its attention on more important features as other tasks will provide additional evidence for the relevance or irrelevance of those features. . (III) Eavesdropping: Some features G may be easy to learn for task B, while being difficult to learn for task A. This may be because A interacts with the features in a more complex or less clear way. Through MTL, we can allow the model to “eavesdrop” (i.e. learn G through task B). . Consider the following two tasks: dog breed detection and giraffe breed detection. One feature that is useful for both tasks is the neck length (the neck length of dogs and giraffes vary across breeds). We will certainly learn this feature in the giraffe breed detection task given how prominent their necks are and the large variation across giraffe breeds. However, we are less likely to do so in the dog breed detection task as differences in neck length are less obvious. To circumvent this, we train the model on both tasks, increasing the probability that we capture this feature for dog breed detection too. . There is a subtle difference between (II) and (III). We can think of (III) as being a more extreme version of (II). In (II), the single-task model can pick up important features but is not sure how important they are. In (III), the single-task model may miss out on these features entirely. . (IV) Representation bias: MTL prefers a generalized representation that extends across multiple tasks. This will allow us to learn new tasks more easily in the future (as long as those new tasks share commonalities with our existing set of tasks). . Negative Transfer . Negative transfer occurs when training individual models on multiple tasks produces better performance than a single MTL model. There are two general cases when this arises: . Firstly, if the tasks are unrelated to each other (e.g. dog detection vs text sentiment analysis). In which case, there are no similarities between the different tasks for MTL to exploit. There may even be a decrease in performance since MTL imposes shared parameters across the different tasks. It would be better to give these distinct tasks full control over their own set of parameters. . Secondly, if the information learnt in one task contradicts that of another task. Consider a dog detection and plant detection task. For the dog detection task, the presence of fur is extremely important. In contrast, for the plant detection task, that feature is irrelevant. If we train the model on both tasks, we can expect to attach a moderate weight to that feature. This weight will be less than the weight that we would learn from training our model on just dog detection. And more than the weight we would learn from training our model on just plant detection. Consequently, the MTL weight is suboptimal for both dog and plant detection. . Conclusion . That’s the end of our primer on multi-task learning! MTL is a powerful and promising technique that allows us to exploit similarities between different related tasks. It does so through parameter sharing. It is important to be discerning about the tasks we include - if not, we may run into the problem of negative transfer (where it is better to train individual models). . Useful Resources . Stanford CS330 lectures | Review paper by Ruder (2017) | Review paper by Zhang and Yang (2021) (note: more technical than Ruder (2017)) | .",
            "url": "https://niclui.github.io/writing/multi-task%20learning/deep%20learning/2021/10/13/A-primer-on-Multi-Task-Learning.html",
            "relUrl": "/multi-task%20learning/deep%20learning/2021/10/13/A-primer-on-Multi-Task-Learning.html",
            "date": " • Oct 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Paper Review - Squeeze & Excitation Networks",
            "content": "A review of Hu et al (2017)’s paper on squeeze and excitation networks. . Note: This blogpost assumes that the reader has a working understanding of convolutional neural networks (CNNs). For a good introduction to CNNs, please see this article. . Introduction . Recap of CNNs . Convolutional neural networks (CNNs) are extremely useful for wide range of visual tasks (e.g. image classification). Here is a quick recap of how CNNs work: . First, we pass in an image of dimensions HxWxC, where H is height, W is width, and C is the number of input channels. For a colored image, C is usually 3 (corresponding to values for RGB). For a grayscale image, C would be 1. | The image is then passed into a convolutional layer which applies a variety of filters onto the image. These filter kernels convolve (slide) across the image, generating feature values for different parts of the image. Each filter kernel will generate a feature map, which is a matrix of feature values for different parts of the image. After applying multiple filter kernels, we will be left with a set of feature maps, U. U has dimensions H’XW’XC’. The height and width of U depends on many factors (e.g. the stride of the filter kernels, or whether we have padded the image). If we use a stride of 1 and pad the image with zeroes all around, our H’ and W’ will be equivalent to H and W respectively. C’ is the number of filter kernels we applied. It is also the number of output channels that we are left with. Here is a fantastic visualisation of the process. | Our set of feature maps, U, is then passed into a non-linear activation function (e.g. ReLU). This allows us to introduce non-linearity into the model, allowing us to capture more complex relationships. Importantly, we also need this non-linear function to decouple our linear layers. Intuitively, a series of linear layers is equivalent to just one linear layer. To decouple the linear layers from one another, we insert a non-linear function in between them. | After we have passed our feature maps, U, into the non-linear function, we pass them into a pooling layer. The purpose of a pooling layer is to downsample the feature maps to reduce the computational load. One example of a pooling function is max pooling, in which we convolve a NXN grid across the feature maps. We then keep only the largest feature value inside the grid. Consequently, we are left with much smaller feature maps, while still retaining information about the “anomalies” (e.g. parts of the image with noticeable edges). | We go through multiple rounds of convolutional layer &gt; non-linear function &gt; pooling layer. The earlier convolutional layers will capture more general patterns and relationships (e.g. edge detection). In contrast, the later convolutional layers will capture patterns that are much more class-specific (e.g. whether the animal in the picture has whiskers). Note that the number of output channels will grow as we go deeper into the CNN (since a feature map is passed into the next convolutional layer and spits out multiple new feature maps). | Finally, we pass our set of feature maps into a fully-connected layer and a final activation function to churn out a final prediction. | . Motivation for SENs . So what exactly is sub-optimal about standard CNNs? Intuitively, there are more important feature maps and less important feature maps. When classifying a shape, the feature map contatining information about edges may be more important than the one containing information about background color. Thus, we will want our model to prioritize these more important feature maps, and deprioritize the less important ones. In technical terms, we want “channel attention” - we want to give greater “attention” to the more important “channels” (aka feature maps). Standard CNNs are unable to do this. Here is where squeeze and excitation networks come in. . Squeeze &amp; Excitement Networks . The seminal paper by Hu et al (2017) propose a novel neural network architecture called squeeze-and-excitement blocks. Squeeze-and-excitement blocks allow us to model inter-dependencies between channels, and to identify which channels are more important than others. There are three stages: . Stage 1: Squeeze Module . We have our set of feature maps, U, which is a tensor with dimensions H’XW’XC’. We want to model the interdependencies between the different channels/feature maps. The problem is that each channel operates within a local receptive field. In other words, each element of a given feature map corresponds with only a specific part of the image. This is problematic as we will want to use global spatial information when computing channel interdependencies - if not, we will not be able to identify the interdependence between say, the value of channel 1 in the top right hand pixel, and that of channel 2 in the bottom left hand pixel. . A simple approach would be to simply use every single feature value in U. While this may improve model performance, it is extremely computationally intensive (we need to work with H’XW’XC’ values and C’ blows up as we go deeper into the neural network). . To mitigate this trade-off, the authors choose to generate channel-wise statistics. We can use average pooling to generate a single value for each channel. In average pooling, we simply take the average value in a given feature map. This allows us to generate a channel descriptor matrix of dimensions 1X1XC’. Each channel will be compressed into a single value. We have thus squeezed our set of feature maps into a compact channel descriptor matrix that contains global spatial information. . Stage 2: Excite Module . In this stage, we want to make use of the channel-wise information aggregated in the channel descriptor in order to calculate the scaling weights for different channels (higher scaling weight = more important). The authors opt for a fully connected multi-layer perceptron (MLP) bottleneck structure to map the scaling weights. A bottleneck structure works as follows: . We pass in our tensor of 1X1XC’. | The input layer has C’ neurons. | The hidden layer has C’/r neurons. Our input space is thus reduced by a factor of r. This hidden layer also introduces non-linearity with a ReLU function. | The output layer has C’ neurons. The compressed space is then expanded back to its original dimensionality. We get back an “excited” output tensor with the same dimensions as the input tensor (1XCXC’). | . To maximize our cross-channel interactions, we should set r to be 1. If r &gt; 1, we lose neurons in the hidden layer (and thus lose out on the granularity of our cross-channel interactions). However, a smaller r also means greater computational complexity (as we have a greater number of parameters to optimize). The default value of the reduction ratio, r, is 16. . Stage 3: Scale Module . The excited weights tensor is then passed into a Sigmoid activation layer to scale the values to a range of 0-1. Subsequently, by broadcasting, multiply the weights tensor with the original set of feature maps to obtain the scaled set of feature maps. Each channel is now scaled by the weight that was learned from the MLP in stage 2. . Where do we fit the squeeze-and-excitation block? . In standard architectures, the SE block can be inserted after the non-linearity following each convolution. Its “plug and play” insertion makes it viable for a wide range of network architectures. . In summary, we pass in an image (X) of HXWXC dimensions into the convolutional layer. The convolutional layer spits out a tensor of H’XW’XC’, which we pass into a non-linearity. After that, we pass the tensor (U) into a squeeze module which squeezes the tensor into a 1X1XC’ tensor. This 1X1XC’ tensor is passed into the excite module and returns a 1X1XC’ “excited” output tensor. The excited output tensor is passed into a Sigmoid function to generate a scaled set of weights. We then multiply these learnt weights with the set of feature maps to scale them. . The overall architecture can be seen below. . . Source: Hu et al (2017) Choice of Architecture . In this section, we will evaluate the authors’ architectural and hyperparameter choices. . 1. Squeeze operator: The authors chose to use global average pooling as the squeeze operator, instead of global max pooling. The former takes the average value in a defined window (in our example, we take the average value across the entire feature map), while the latter takes the maximum value in a defined window. . Max pooling allows us to preserve the most activated pixels in an image (since we preserve the highest value of the feature map). These pixels tend to be the most important or class-deterministic pixels. However, it can be extremely noisy (similiar images may have very different maximum values). It also ignores the effect of neighboring pixels. . Average pooling allows us to construct a smooth average of values in a feature map. It is less noisy (similiar images won’t differ by large amounts) and takes into account neighboring pixels. The downside is that we lose information about the most activating pixels. Average pooling does not discriminate between important/class-deterministic pixels and pixels which do not contain useful information. Instead, it aggregates all these pixels together - we capture aggregate spatial information, but do not preserve information on the most important pixels. . The authors eventually settled on average pooling as an ablation study showed that global average pooling produced a smaller error rate. . However, more recent innovations have tried to combine different methods of pooling to obtain better results. In Convolutional Block Attention Module (CBAM), the input tensor is decomposed into two C’X1X1 vectors - one is generated by global average pooling, while the other is generated by global max pooling. Global average pooling preserves aggregate spatial information, while global max pooling preserves the most activating pixels. . 2. Reduction ratio, r: The smaller the value of r, the more neurons we retain in the bottleneck. This allows us to capture more granular cross-channel interactions, improving the model performance. However, it also means that we have a greater number of parameters to train, increasing computational complexity. However, the authors find that when r decreases, performance does not increase monotonically (i.e. it may be plateauing or even decreasing at some points). They find that r = 16 provides a good balance between accuracy and complexity. . However, note that it may not be optimal to maintain the same value of r throughout the network. For instance, there may be more complex channel interdependencies at later layers. We might want to use a lower value of r there to capture the more complex relationships. . 3. Excitation Operator An ablation study shows that Sigmoid is the best excitation operator. Other options (tanh, ReLU) significantly decreases performance. . Overall Evaluation . By modelling channel interdependencies, SENets produce a huge improvement in model performance at slight computational cost. However, it still faces a few limitations: . To reduce computational complexity, we used a bottleneck MLP in our excitation module. This causes information loss. | SENets use Global Average Pooling (GAP) to squeeze the set of feature maps into a channel descriptor matrix. While GAP can aggregate spatial information, it does not preserve the most activated pixels. There have been new alternatives, such as CBAM (see above), which help to rectify this problem. | SENets add a large amount of parameters (~3m more parameters in a SENet vs ResNet-50). The computational cost is still manageable. However, there have been newer approaches which provide as good a performance at SENets (if not better) at lower computational cost. One example is ECANet, which show that “avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity.” | . (Cover picture: Hu et al (2017)) .",
            "url": "https://niclui.github.io/writing/paper%20review/channel%20attention/deep%20learning/2021/09/12/Paper-Review-Squeeze-&-Excitation-Networks.html",
            "relUrl": "/paper%20review/channel%20attention/deep%20learning/2021/09/12/Paper-Review-Squeeze-&-Excitation-Networks.html",
            "date": " • Sep 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "A gentle introduction to gradient descent",
            "content": "Gradient descent is one of the key foundations of machine learning. Today, I hope to give a high-level and intuitive overview of this concept. . Gradient descent . Background . Recall the process of training a machine learning model. The steps broadly involve: . Initializing a bunch of parameters | Using those parameters to make predictions from your input | Based on the quality of your predictions, tweaking your parameters to make the model better | Repeating steps 2 and 3 until you are satisfied with your model (e.g. when the overall prediction error is below a certain threshold). | . Gradient descent (GD) is an algorithm that helps us accomplish these steps. To illustrate this concept, let us imagine that we are building a model which can help us distinguish between two handwritten digits - say, the numbers 3 and 7. We are training our model on a large dataset of handwritten 3s and 7s - for simplicity, the pictures are all grayscale and have the same dimensions. . . Source: 1001 Free Downloads How does gradient descent work? . Step 1: Initializing a bunch of parameters . Our parameters can be the weights attached to every feature of the picture (lets say that we have 100 features). We start off by assigning a random weight to each feature. . Step 2: Using those parameters to make predictions from your input . We can multiply the value of every feature with its weight. After that, get the average weighted value across all features. Following which, pass that value into a special function (e.g. Sigmoid function) to generate a probability between 0 and 1. If that probability is &gt;0.5, predict a 3. If not, predict a 7. . Step 3: Tweaking your parameters . The magic of gradient descent happens here. Intuitively, we want to update our parameters in a way that will make our predictions better. . Let’s imagine that we are updating the weight of feature m. We plot this weight on the x-axis. On the y-axis, we will plot the loss function with respect to that weight. What is the loss function? Essentially, it is a function that will return a small value when your model is good, and a large value when your model is bad. An example of a loss function is binary cross entropy. This loss function will give us a small value when we are making a correct prediction with high confidence, a medium value when we are making a correct prediction with poor confidence, and a large value when we are making a wrong prediction. . Gradient descent involves us taking iterative steps to find the local minima of the loss function (with respect to the parameter we are adjusting). Consider the case of a convex loss function (see picture below). Let’s say that we initialize our weight at a value of 1. The GD algorithm will calculate the gradient at that point. To calculate the gradient, we can adjust the weight by a tiny margin (holding other weights constant) and see the impact that it has on the loss function. By dividing the change in loss by the change in weight, we get the gradient. . The algorithm will then increase/decrease the value of x by the value of the gradient multiplied by a specified learning rate. Since the gradient is negative (i.e. we will reduce our loss by increasing our weight), the algorithm will increase the weight by the value of gradient * the learning rate. . . Source: fast.ai Tip: The learning rate controls the rate at which the model adjusts its parameters. Selecting the optimal learning rate is tricky. If we select an overly large rate, the model will adjust the parameters by huge amounts, potentially resulting in us bypassing the local minima. In contrast, if the learning rate is too small, it will take a long time to reach the local minima. There are several ways to tune this important hyperparameter. For instance, you could do learning rate annealing. Start off with a high learning rate so that you can quickly descend to an acceptable set of parameter values. After that, decrease your learning rate so that you can precisely locate the optimal value within the acceptable range. Here’s a good article which explains more. . Step 4: Iterate until you are satisfied . Eventually, after multiple rounds of iteration, we will reach the local minima of the loss function. At this point, our gradient is zero and any further adjustments to the weight will increase loss. . . Source: fast.ai We repeat this process for every weight (all 100 of them!). . A common analogy for gradient descent is that of a blindfolded hiker who is stuck on the side of a hill. He wants to get to as low a point as possible. Thus, he feels the ground around him and takes a small step in the steepest downward direction. This is one iteration. By taking many of these small steps, he will eventually reach the bottom of a valley (local minima). . . Source: inspiredbymaps Tip: Note that gradient descent requires our loss function to be continuous and smooth. What if our loss function is discontinuous - say, a step function? To illustrate this, initialize our weight at 1. The gradient at that point is completely flat. Thus, the GD algorithm will terminate immediately. However, if we increased our weight by a larger amount, we could have moved to a lower step of the loss function. In this instance, the GD algorithm will not give us good results and the model will not learn well. This is why we cannot use accuracy (the % of correct classifications) as our loss function. We can imagine accuracy to be represented by a step function - if we change a weight by a tiny amount, we do not expect any prediction to change from a 3 to 7 (or vice versa). As such, accuracy remains unchanged. A much larger adjustment in a weight is needed to induce a change in our predictions. Consequently, we use a continuous loss function which improves when we make correct predictions with slightly more confidence, or make wrong predictions with slightly less confidence. . . Source: Desmos What are the limitations of gradient descent? . There are two key limitations behind standard gradient descent (or batch gradient descent): . Firstly, it is possible that we can get stuck in a local minima of the loss function, preventing us from accessing the better global minima. Consider the illustration below. We start at point U and adjust iteratively until we hit the local minima and the GD algorithm terminates. | . . Source: Analytics Vidhya Secondly, in standard gradient descent, we use every single datapoint in our training set to compute gradients. Let’s say we have 5,000 training images. For a given parameter, we use all 5,000 images to calculate individual losses and take the mean. After that, we adjust the parameter value slightly and re-calculate the loss for all 5,000 images (taking the mean again). The difference in means divided by the difference in weight is the gradient. When the training set is large, this process becomes computationally intensive. | . Other flavours of gradient descent . Stochastic gradient descent . To remedy those limitations, stochastic gradient descent is a popular alternative. In stochastic gradient descent, we do not use the entire training dataset in our computation of gradients. Instead, in each iteration, we randomly select a training datapoint to do so. This gives us a stochastic (i.e. random) approximation to the gradient calculated with the entire training dataset. By constantly iterating, the parameter value should move in the same general direction (as standard gradient descent), while being computationally more efficient. . The randomness can also allow us to escape local minima. Every training datapoint has its own unique loss function. In standard gradient descent, we average all these loss functions into one aggregate loss function. In unfortunate cases, we will move down that average loss function into a sub-optimal local minima where we are trapped forever. . However, in stochastic gradient descent, we can potentially avoid this negative outcome. In each iteration, we hop from the loss function of one training datapoint to another. Even if we get stuck in the local minima of a loss function, we can move to a new loss function in the next iteration (where our current parameter value is not in a local minima anymore). This allows us to keep moving and iterating. . Stochastic gradient descent faces two key weaknesses: . The stochastic steps it take can be very noisy. This may result in us taking more time to converge to the local minima of the loss function (ignoring the case where the local minima may be sub-optimal). Stochastic gradient descent is computationally more efficient, but may end up taking more time. | Computing infrastructure in ML (e.g. GPUs) are optimised for vectorized operations (vector addition, multiplication). Given that we use only one training datapoint at a time, we give up this powerful capability. | . Mini-batch gradient descent . Mini-batch gradient descent is a compromise between standard gradient descent and stochastic gradient descent. We do not use the entire training dataset or a single training datapoint. Instead, we use small batches (typically ~30-500) of training datapoints to compute our gradient. Given that we use small batches of datapoints at a time, we are also able to harness the performance of GPUs in vectorized operations. . Conclusion . I hope this article has given you a good overview of the key concepts in gradient descent! Here’s a quick summary: . Gradient descent is an algorithm that allows us to iteratively adjust our model’s parameters for better performance. We adjust our parameters in a direction that brings us down the loss function. | The two biggest limitations of standard gradient descent are 1) the possibility that we may be trapped in the local minima of our loss function, and 2) the computational cost. | Stochastic and mini-batch gradient descent are popular alternatives that mitigate some of these issues. | . Hope you enjoyed reading! . (Cover picture credit: inspiredbymaps) .",
            "url": "https://niclui.github.io/writing/gradient%20descent/deep%20learning/2021/09/04/A-gentle-introduction-to-gradient-descent.html",
            "relUrl": "/gradient%20descent/deep%20learning/2021/09/04/A-gentle-introduction-to-gradient-descent.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Nice to meet you! I am Nicholas, a Statistics MS student and Knight-Hennessy Scholar at Stanford University. I currently work on deep learning problems in climate change under Andrew Ng’s machine learning research group. . For my undergrad, I studied Economics at Cambridge (the best three years of my life). I have worked on anti-competitive behavior detection at GovTech, and on ML-driven pricing strategy at Travelstart. I also developed product strategy at ByteDance as the first intern (and sixth employee) of its Global B2B business. . In my free time, I box, run, and watch Netflix (trying to find a new show to binge watch!). I am also a strong advocate for mindfulness, and its value in helping us become kinder to ourselves and others. Here’s a great resource if you are interested. . If you would like to chat about anything, please drop me a line via email or LinkedIn. Thank you for visiting :) . . California dreamin&#39; (Oct 2021)",
          "url": "https://niclui.github.io/writing/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Home",
          "content": "",
          "url": "https://niclui.github.io/writing/",
          "relUrl": "/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://niclui.github.io/writing/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}