{
  
    
        "post0": {
            "title": "Paper Review - Squeeze & Excitation Networks",
            "content": "A review of Hu et al (2017)’s paper on squeeze and excitation networks. . Note: This blogpost assumes that the reader has a working understanding of convolutional neural networks (CNNs). For a good introduction to CNNs, please see this article. . Introduction . Recap of CNNs . Convolutional neural networks (CNNs) are extremely useful for wide range of visual tasks (e.g. image classification). Here is a quick recap of how CNNs work: . First, we pass in an image of dimensions HxWxC, where H is height, W is width, and C is the number of input channels. For a colored image, C is usually 3 (corresponding to values for RGB). For a grayscale image, C would be 1. | The image is then passed into a convolutional layer which applies a variety of filters onto the image. These filter kernels convolve (slide) across the image, generating feature values for different parts of the image. Each filter kernel will generate a feature map, which is a matrix of feature values for different parts of the image. After applying multiple filter kernels, we will be left with a set of feature maps, U. U has dimensions H’XW’XC’. The height and width of U depends on many factors (e.g. the stride of the filter kernels, or whether we have padded the image). If we use a stride of 1 and pad the image with zeroes all around, our H’ and W’ will be equivalent to H and W respectively. C’ is the number of filter kernels we applied. It is also the number of output channels that we are left with. Here is a fantastic visualisation of the process. | Our set of feature maps, U, is then passed into a non-linear activation function (e.g. ReLU). This allows us to introduce non-linearity into the model, allowing us to capture more complex relationships. Importantly, we also need this non-linear function to decouple our linear layers. Intuitively, a series of linear layers is equivalent to just one linear layer. To decouple the linear layers from one another, we insert a non-linear function in between them. | After we have passed our feature maps, U, into the non-linear function, we pass them into a pooling layer. The purpose of a pooling layer is to downsample the feature maps to reduce the computational load. One example of a pooling function is max pooling, in which we convolve a NXN grid across the feature maps. We then keep only the largest feature value inside the grid. Consequently, we are left with much smaller feature maps, while still retaining information about the “anomalies” (e.g. parts of the image with noticeable edges). | We go through multiple rounds of convolutional layer &gt; non-linear function &gt; pooling layer. The earlier convolutional layers will capture more general patterns and relationships (e.g. edge detection). In contrast, the later convolutional layers will capture patterns that are much more class-specific (e.g. whether the animal in the picture has whiskers). Note that the number of output channels will grow as we go deeper into the CNN (since a feature map is passed into the next convolutional layer and spits out multiple new feature maps). | Finally, we pass our set of feature maps into a fully-connected layer and a final activation function to churn out a final prediction. | . Motivation for SENs . So what exactly is sub-optimal about standard CNNs? Intuitively, there are more important feature maps and less important feature maps. When classifying a shape, the feature map contatining information about edges may be more important than the one containing information about background color. Thus, we will want our model to prioritize these more important feature maps, and deprioritize the less important ones. In technical terms, we want “channel attention” - we want to give greater “attention” to the more important “channels” (aka feature maps). Standard CNNs are unable to do this. Here is where squeeze and excitation networks come in. . Squeeze &amp; Excitement Networks . The seminal paper by Hu et al (2017) propose a novel neural network architecture called squeeze-and-excitement blocks. Squeeze-and-excitement blocks allow us to model inter-dependencies between channels, and to identify which channels are more important than others. There are three stages: . Stage 1: Squeeze Module . We have our set of feature maps, U, which is a tensor with dimensions H’XW’XC’. We want to model the interdependencies between the different channels/feature maps. The problem is that each channel operates within a local receptive field. In other words, each element of a given feature map corresponds with only a specific part of the image. This is problematic as we will want to use global spatial information when computing channel interdependencies - if not, we will not be able to identify the interdependence between say, the value of channel 1 in the top right hand pixel, and that of channel 2 in the bottom left hand pixel. . A simple approach would be to simply use every single feature value in U. While this may improve model performance, it is extremely computationally intensive (we need to work with H’XW’XC’ values and C’ blows up as we go deeper into the neural network). . To mitigate this trade-off, the authors choose to generate channel-wise statistics. We can use average pooling to generate a single value for each channel. In average pooling, we simply take the average value in a given feature map. This allows us to generate a channel descriptor matrix of dimensions 1X1XC’. Each channel will be compressed into a single value. We have thus squeezed our set of feature maps into a compact channel descriptor matrix that contains global spatial information. . Stage 2: Excite Module . In this stage, we want to make use of the channel-wise information aggregated in the channel descriptor in order to calculate the scaling weights for different channels (higher scaling weight = more important). The authors opt for a fully connected multi-layer perceptron (MLP) bottleneck structure to map the scaling weights. A bottleneck structure works as follows: . We pass in our tensor of 1X1XC’. | The input layer has C’ neurons. | The hidden layer has C’/r neurons. Our input space is thus reduced by a factor of r. This hidden layer also introduces non-linearity with a ReLU function. | The output layer has C’ neurons. The compressed space is then expanded back to its original dimensionality. We get back an “excited” output tensor with the same dimensions as the input tensor (1XCXC’). | . To maximize our cross-channel interactions, we should set r to be 1. If r &gt; 1, we lose neurons in the hidden layer (and thus lose out on the granularity of our cross-channel interactions). However, a smaller r also means greater computational complexity (as we have a greater number of parameters to optimize). The default value of the reduction ratio, r, is 16. . Stage 3: Scale Module . The excited weights tensor is then passed into a Sigmoid activation layer to scale the values to a range of 0-1. Subsequently, by broadcasting, multiply the weights tensor with the original set of feature maps to obtain the scaled set of feature maps. Each channel is now scaled by the weight that was learned from the MLP in stage 2. . Where do we fit the squeeze-and-excitation block? . In standard architectures, the SE block can be inserted after the non-linearity following each convolution. Its “plug and play” insertion makes it viable for a wide range of network architectures. . In summary, we pass in an image (X) of HXWXC dimensions into the convolutional layer. The convolutional layer spits out a tensor of H’XW’XC’, which we pass into a non-linearity. After that, we pass the tensor (U) into a squeeze module which squeezes the tensor into a 1X1XC’ tensor. This 1X1XC’ tensor is passed into the excite module and returns a 1X1XC’ “excited” output tensor. The excited output tensor is passed into a Sigmoid function to generate a scaled set of weights. We then multiply these learnt weights with the set of feature maps to scale them. . The overall architecture can be seen below. . . Source: Hu et al (2017) Choice of Architecture . In this section, we will evaluate the authors’ architectural and hyperparameter choices. . 1. Squeeze operator: The authors chose to use global average pooling as the squeeze operator, instead of global max pooling. The former takes the average value in a defined window (in our example, we take the average value across the entire feature map), while the latter takes the maximum value in a defined window. . Max pooling allows us to preserve the most activated pixels in an image (since we preserve the highest value of the feature map). These pixels tend to be the most important or class-deterministic pixels. However, it can be extremely noisy (similiar images may have very different maximum values). It also ignores the effect of neighboring pixels. . Average pooling allows us to construct a smooth average of values in a feature map. It is less noisy (similiar images won’t differ by large amounts) and takes into account neighboring pixels. The downside is that we lose information about the most activating pixels. Average pooling does not discriminate between important/class-deterministic pixels and pixels which do not contain useful information. Instead, it aggregates all these pixels together - we capture aggregate spatial information, but do not preserve information on the most important pixels. . The authors eventually settled on average pooling as an ablation study showed that global average pooling produced a smaller error rate. . However, more recent innovations have tried to combine different methods of pooling to obtain better results. In Convolutional Block Attention Module (CBAM), the input tensor is decomposed into two C’X1X1 vectors - one is generated by global average pooling, while the other is generated by global max pooling. Global average pooling preserves aggregate spatial information, while global max pooling preserves the most activating pixels. . 2. Reduction ratio, r: The smaller the value of r, the more neurons we retain in the bottleneck. This allows us to capture more granular cross-channel interactions, improving the model performance. However, it also means that we have a greater number of parameters to train, increasing computational complexity. However, the authors find that when r decreases, performance does not increase monotonically (i.e. it may be plateauing or even decreasing at some points). They find that r = 16 provides a good balance between accuracy and complexity. . However, note that it may not be optimal to maintain the same value of r throughout the network. For instance, there may be more complex channel interdependencies at later layers. We might want to use a lower value of r there to capture the more complex relationships. . 3. Excitation Operator An ablation study shows that Sigmoid is the best excitation operator. Other options (tanh, ReLU) significantly decreases performance. . Overall Evaluation . By modelling channel interdependencies, SENets produce a huge improvement in model performance at slight computational cost. However, it still faces a few limitations: . To reduce computational complexity, we used a bottleneck MLP in our excitation module. This causes information loss. | SENets use Global Average Pooling (GAP) to squeeze the set of feature maps into a channel descriptor matrix. While GAP can aggregate spatial information, it does not preserve the most activated pixels. There have been new alternatives, such as CBAM (see above), which help to rectify this problem. | SENets add a large amount of parameters (~3m more parameters in a SENet vs ResNet-50). The computational cost is still manageable. However, there have been newer approaches which provide as good a performance at SENets (if not better) at lower computational cost. One example is ECANet, which show that “avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity.” | . (Cover picture: Hu et al (2017)) .",
            "url": "https://niclui.github.io/writing/paper%20review/channel%20attention/deep%20learning/2021/09/12/Paper-Review-Squeeze-&-Excitation-Networks.html",
            "relUrl": "/paper%20review/channel%20attention/deep%20learning/2021/09/12/Paper-Review-Squeeze-&-Excitation-Networks.html",
            "date": " • Sep 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A gentle introduction to gradient descent",
            "content": "Gradient descent is one of the key foundations of machine learning. Today, I hope to give a high-level and intuitive overview of this concept. . Gradient descent . Background . Recall the process of training a machine learning model. The steps broadly involve: . Initializing a bunch of parameters | Using those parameters to make predictions from your input | Based on the quality of your predictions, tweaking your parameters to make the model better | Repeating steps 2 and 3 until you are satisfied with your model (e.g. when the overall prediction error is below a certain threshold). | . Gradient descent (GD) is an algorithm that helps us accomplish these steps. To illustrate this concept, let us imagine that we are building a model which can help us distinguish between two handwritten digits - say, the numbers 3 and 7. We are training our model on a large dataset of handwritten 3s and 7s - for simplicity, the pictures are all grayscale and have the same dimensions. . . Source: 1001 Free Downloads How does gradient descent work? . Step 1: Initializing a bunch of parameters . Our parameters can be the weights attached to every feature of the picture (lets say that we have 100 features). We start off by assigning a random weight to each feature. . Step 2: Using those parameters to make predictions from your input . We can multiply the value of every feature with its weight. After that, get the average weighted value across all features. Following which, pass that value into a special function (e.g. Sigmoid function) to generate a probability between 0 and 1. If that probability is &gt;0.5, predict a 3. If not, predict a 7. . Step 3: Tweaking your parameters . The magic of gradient descent happens here. Intuitively, we want to update our parameters in a way that will make our predictions better. . Let’s imagine that we are updating the weight of feature m. We plot this weight on the x-axis. On the y-axis, we will plot the loss function with respect to that weight. What is the loss function? Essentially, it is a function that will return a small value when your model is good, and a large value when your model is bad. An example of a loss function is binary cross entropy. This loss function will give us a small value when we are making a correct prediction with high confidence, a medium value when we are making a correct prediction with poor confidence, and a large value when we are making a wrong prediction. . Gradient descent involves us taking iterative steps to find the local minima of the loss function (with respect to the parameter we are adjusting). Consider the case of a convex loss function (see picture below). Let’s say that we initialize our weight at a value of 1. The GD algorithm will calculate the gradient at that point. To calculate the gradient, we can adjust the weight by a tiny margin (holding other weights constant) and see the impact that it has on the loss function. By dividing the change in loss by the change in weight, we get the gradient. . The algorithm will then increase/decrease the value of x by the value of the gradient multiplied by a specified learning rate. Since the gradient is negative (i.e. we will reduce our loss by increasing our weight), the algorithm will increase the weight by the value of gradient * the learning rate. . . Source: fast.ai Tip: The learning rate controls the rate at which the model adjusts its parameters. Selecting the optimal learning rate is tricky. If we select an overly large rate, the model will adjust the parameters by huge amounts, potentially resulting in us bypassing the local minima. In contrast, if the learning rate is too small, it will take a long time to reach the local minima. There are several ways to tune this important hyperparameter. For instance, you could do learning rate annealing. Start off with a high learning rate so that you can quickly descend to an acceptable set of parameter values. After that, decrease your learning rate so that you can precisely locate the optimal value within the acceptable range. Here’s a good article which explains more. . Step 4: Iterate until you are satisfied . Eventually, after multiple rounds of iteration, we will reach the local minima of the loss function. At this point, our gradient is zero and any further adjustments to the weight will increase loss. . . Source: fast.ai We repeat this process for every weight (all 100 of them!). . A common analogy for gradient descent is that of a blindfolded hiker who is stuck on the side of a hill. He wants to get to as low a point as possible. Thus, he feels the ground around him and takes a small step in the steepest downward direction. This is one iteration. By taking many of these small steps, he will eventually reach the bottom of a valley (local minima). . . Source: inspiredbymaps Tip: Note that gradient descent requires our loss function to be continuous and smooth. What if our loss function is discontinuous - say, a step function? To illustrate this, initialize our weight at 1. The gradient at that point is completely flat. Thus, the GD algorithm will terminate immediately. However, if we increased our weight by a larger amount, we could have moved to a lower step of the loss function. In this instance, the GD algorithm will not give us good results and the model will not learn well. This is why we cannot use accuracy (the % of correct classifications) as our loss function. We can imagine accuracy to be represented by a step function - if we change a weight by a tiny amount, we do not expect any prediction to change from a 3 to 7 (or vice versa). As such, accuracy remains unchanged. A much larger adjustment in a weight is needed to induce a change in our predictions. Consequently, we use a continuous loss function which improves when we make correct predictions with slightly more confidence, or make wrong predictions with slightly less confidence. . . Source: Desmos What are the limitations of gradient descent? . There are two key limitations behind standard gradient descent (or batch gradient descent): . Firstly, it is possible that we can get stuck in a local minima of the loss function, preventing us from accessing the better global minima. Consider the illustration below. We start at point U and adjust iteratively until we hit the local minima and the GD algorithm terminates. | . . Source: Analytics Vidhya Secondly, in standard gradient descent, we use every single datapoint in our training set to compute gradients. Let’s say we have 5,000 training images. For a given parameter, we use all 5,000 images to calculate individual losses and take the mean. After that, we adjust the parameter value slightly and re-calculate the loss for all 5,000 images (taking the mean again). The difference in means divided by the difference in weight is the gradient. When the training set is large, this process becomes computationally intensive. | . Other flavours of gradient descent . Stochastic gradient descent . To remedy those limitations, stochastic gradient descent is a popular alternative. In stochastic gradient descent, we do not use the entire training dataset in our computation of gradients. Instead, in each iteration, we randomly select a training datapoint to do so. This gives us a stochastic (i.e. random) approximation to the gradient calculated with the entire training dataset. By constantly iterating, the parameter value should move in the same general direction (as standard gradient descent), while being computationally more efficient. . The randomness can also allow us to escape local minima. Every training datapoint has its own unique loss function. In standard gradient descent, we average all these loss functions into one aggregate loss function. In unfortunate cases, we will move down that average loss function into a sub-optimal local minima where we are trapped forever. . However, in stochastic gradient descent, we can potentially avoid this negative outcome. In each iteration, we hop from the loss function of one training datapoint to another. Even if we get stuck in the local minima of a loss function, we can move to a new loss function in the next iteration (where our current parameter value is not in a local minima anymore). This allows us to keep moving and iterating. . Stochastic gradient descent faces two key weaknesses: . The stochastic steps it take can be very noisy. This may result in us taking more time to converge to the local minima of the loss function (ignoring the case where the local minima may be sub-optimal). Stochastic gradient descent is computationally more efficient, but may end up taking more time. | Computing infrastructure in ML (e.g. GPUs) are optimised for vectorized operations (vector addition, multiplication). Given that we use only one training datapoint at a time, we give up this powerful capability. | . Mini-batch gradient descent . Mini-batch gradient descent is a compromise between standard gradient descent and stochastic gradient descent. We do not use the entire training dataset or a single training datapoint. Instead, we use small batches (typically ~30-500) of training datapoints to compute our gradient. Given that we use small batches of datapoints at a time, we are also able to harness the performance of GPUs in vectorized operations. . Conclusion . I hope this article has given you a good overview of the key concepts in gradient descent! Here’s a quick summary: . Gradient descent is an algorithm that allows us to iteratively adjust our model’s parameters for better performance. We adjust our parameters in a direction that brings us down the loss function. | The two biggest limitations of standard gradient descent are 1) the possibility that we may be trapped in the local minima of our loss function, and 2) the computational cost. | Stochastic and mini-batch gradient descent are popular alternatives that mitigate some of these issues. | . Hope you enjoyed reading! . (Cover picture credit: inspiredbymaps) .",
            "url": "https://niclui.github.io/writing/gradient%20descent/deep%20learning/2021/09/04/A-gentle-introduction-to-gradient-descent.html",
            "relUrl": "/gradient%20descent/deep%20learning/2021/09/04/A-gentle-introduction-to-gradient-descent.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Nice to meet you! I am Nicholas, a Statistics MS student and Knight-Hennessy Scholar at Stanford University. I currently work on deep learning problems in climate change under Andrew Ng’s machine learning research group. For my undergrad, I studied Economics at Cambridge (the best three years of my life). I have worked on anti-competitive behavior detection at GovTech, and on ML-driven pricing strategy at Travelstart. I also developed product strategy at ByteDance as the first intern (and sixth employee) of its Global B2B unit. In another life, I led an infantry platoon in counter-terrorism operations as a Platoon Sergeant. . In my free time, I box, run, and watch Netflix (trying to find a new show to binge watch!). I am also a strong advocate for mindfulness, and its value in helping us become kinder to ourselves and others. Here’s a great resource if you are interested. . If you would like to chat about anything, please drop me a line via email or LinkedIn. Thank you for visiting :) . . On the rustic streets of Athens (Dec 2019)",
          "url": "https://niclui.github.io/writing/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Home",
          "content": "",
          "url": "https://niclui.github.io/writing/",
          "relUrl": "/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://niclui.github.io/writing/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}