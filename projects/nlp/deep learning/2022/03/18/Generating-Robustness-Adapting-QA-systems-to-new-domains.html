<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Generating Robustness - Adapting QA systems to new domains | An adventure into AI</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Generating Robustness - Adapting QA systems to new domains" />
<meta name="author" content="Nicholas" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A project that my team worked on for our Stanford CS224N project. We implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a 16% increase in F1 score in development and 10% increase in test." />
<meta property="og:description" content="A project that my team worked on for our Stanford CS224N project. We implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a 16% increase in F1 score in development and 10% increase in test." />
<link rel="canonical" href="https://niclui.github.io/writing/projects/nlp/deep%20learning/2022/03/18/Generating-Robustness-Adapting-QA-systems-to-new-domains.html" />
<meta property="og:url" content="https://niclui.github.io/writing/projects/nlp/deep%20learning/2022/03/18/Generating-Robustness-Adapting-QA-systems-to-new-domains.html" />
<meta property="og:site_name" content="An adventure into AI" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-18T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://niclui.github.io/writing/projects/nlp/deep%20learning/2022/03/18/Generating-Robustness-Adapting-QA-systems-to-new-domains.html","@type":"BlogPosting","headline":"Generating Robustness - Adapting QA systems to new domains","dateModified":"2022-03-18T00:00:00-05:00","datePublished":"2022-03-18T00:00:00-05:00","author":{"@type":"Person","name":"Nicholas"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://niclui.github.io/writing/projects/nlp/deep%20learning/2022/03/18/Generating-Robustness-Adapting-QA-systems-to-new-domains.html"},"description":"A project that my team worked on for our Stanford CS224N project. We implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a 16% increase in F1 score in development and 10% increase in test.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/writing/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://niclui.github.io/writing/feed.xml" title="An adventure into AI" /><link rel="shortcut icon" type="image/x-icon" href="/writing/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/writing/">An adventure into AI</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/writing/about/">About Me</a><a class="page-link" href="/writing/">Home</a><a class="page-link" href="/writing/search/">Search</a><a class="page-link" href="/writing/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Generating Robustness - Adapting QA systems to new domains</h1><p class="page-description">A project that my team worked on for our Stanford CS224N project. We implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a 16% increase in F1 score in development and 10% increase in test.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-18T00:00:00-05:00" itemprop="datePublished">
        Mar 18, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nicholas</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/writing/categories/#projects">projects</a>
        &nbsp;
      
        <a class="category-tags-link" href="/writing/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/writing/categories/#deep learning">deep learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#motivation">Motivation</a></li>
<li class="toc-entry toc-h1"><a href="#setup">Setup</a></li>
<li class="toc-entry toc-h1"><a href="#baseline">Baseline</a></li>
<li class="toc-entry toc-h1"><a href="#techniques">Techniques</a>
<ul>
<li class="toc-entry toc-h2"><a href="#adversarial-learning">Adversarial Learning</a></li>
<li class="toc-entry toc-h2"><a href="#using-out-of-domain-data-in-training-and-fine-tuning">Using out-of-domain data in training and fine-tuning</a></li>
<li class="toc-entry toc-h2"><a href="#data-augmentation">Data Augmentation</a></li>
<li class="toc-entry toc-h2"><a href="#domain-alignment">Domain Alignment</a></li>
<li class="toc-entry toc-h2"><a href="#tuning-discriminator-architecture">Tuning Discriminator Architecture</a></li>
<li class="toc-entry toc-h2"><a href="#ensemble-methods">Ensemble Methods</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#overall-results--insights">Overall Results &amp; Insights</a></li>
<li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li>
</ul><p>I would like to express my deep gratitude to my teammates (Helen Gu and Quentin Hsu) for their collaborative spirit, research ambition, and willingness to tackle open-ended problems
with tenacity. Here is our
<a href="https://drive.google.com/file/d/1-cleNk6Auyrk2rEEW7fBM30FPiLhYORX/view?usp=sharing">project report</a>
and <a href="https://drive.google.com/file/d/1qyAD_KEot7g21jRoFcN6Val0RHBb_1Z1/view?usp=sharing">poster</a>.</p>

<h1 id="motivation">
<a class="anchor" href="#motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h1>
<p>Question and Answering (QA) systems are systems that can automatically answer human questions in a natural language.
They are ubiquitous in everyday life. From the Siri voice assistant on your iPhone to intelligent chatbots, QA systems provide us with
greater convenience, allowing us to access information in an intuitive and personal manner.</p>

<p>The big issue is that QA systems are not robust to domain shifts, diminishing its ability to generalize to domains that it has not been trained on. Let’s say that you are a government agency designing an intelligent chatbot that can answer citizens’
questions on government schemes. Citizens ask a question (“what is the age requirement for the new housing subsidy?”) and the QA system extracts the answer (“21”) from a corpus of information
about the policy (the “context”). The issue is that information about different policies are structured in different ways. The QA system may have been trained on context-question
pairs on housing policy and thus has a good grasp of how information on housing policy is structured, enabling it to perform efficient extractions. However, when it is faced with
context-question pairs from a new policy area (say, financial assistance schemes), it performs poorly as it has little information on the structure and characteristics of information
in that area.</p>

<p>In an ideal world, we would be able to plug this gap by simply getting more labelled data from the new domain and finetuning our QA model on it. However, labelled QA data
is hard to come by and extremely expensive to create (think about the number of man hours needed to create new context-question pairs). As such, we need to explore new techniques
to build robustness in QA systems, allowing them to generalize to unseen domains.</p>

<p>In our project, we implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a <strong>16% increase in F1 score in development</strong> and <strong>10% increase in test</strong>. We find that the following innovations boost model performance: 1) finetuning the model on augmented out-of-domain data, 2) redefining domains during adversarial training to simplify the domain discriminator’s task, and 3) supplementing the training data with synthetic QA pairs generated with roundtrip consistency. We also ensemble the best-performing models on each dataset and find that ensembling yields further performance increases.</p>

<h1 id="setup">
<a class="anchor" href="#setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup</h1>
<p>We are given 3 in-domain training datasets, each with 50,000 samples. The datasets are SQuAD (Wikipedia articles), NaturalQuestions (Wikipedia articles), and NewsQA (news articles). We are also given 3 out-of-domain training datasets, each with approximately 100 samples. The datasets are RelationExtraction (Wikipedia articles), DuoRC (movie reviews), and RACE (examination questions). Performance is evaluated on the out-of-domain validation set.</p>

<h1 id="baseline">
<a class="anchor" href="#baseline" aria-hidden="true"><span class="octicon octicon-link"></span></a>Baseline</h1>
<p>Our baseline model is a DistilBERT trained solely on in-domain data. It achieves an F1 score of 49.88. In the sections below, I will talk about the techniques we employed, followed by any interesting insights we gleaned.</p>

<h1 id="techniques">
<a class="anchor" href="#techniques" aria-hidden="true"><span class="octicon octicon-link"></span></a>Techniques</h1>

<h2 id="adversarial-learning">
<a class="anchor" href="#adversarial-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adversarial Learning</h2>
<p>The core technique in our paper is domain adversarial learning, which we adapt from <a href="https://aclanthology.org/D19-5826/">Lee et al (2019)</a>. The domain adversarial learning model is comprised of two components - the QA model (our DistilBERT) and a domain discriminator. During training, the discriminator is trained to predict the domain (dataset) of the
hidden representation produced by the QA model. In contrast, the QA model is penalized for the
success of the discriminator, thus forcing the QA model to learn domain-invariant features such that
it produces a hidden representation that is indistinguishable to the domain discriminator. At the same
time, the domain discriminator also needs to learn what domain-invariant features to keep in order to
maintain its performance on the samples generated by the QA model.</p>

<p>A good analogy is the Angry Birds game! The domain discriminator is the catapult, while the QA model is the tower. The domain discriminator tries to break down the QA tower (guess what domain the QA hidden representation comes from), and the QA model tries to improve its defence against the attacks of the discriminator catapult (learn domain-invariant features to produce hidden representations that are indistinguishable to the discriminator). The end result is a reinforced QA model tower that is adept at learning domain-invariant features.</p>

<p><img width="60%" alt="space" src="https://user-images.githubusercontent.com/40440105/159158048-0bf596c7-1dbd-463d-8d2b-a8acb89659b9.png"></p>
<center><em>Infographic from our poster</em></center>

<p>The discriminator is trained with a cross-entropy loss function. For a given training point, the loss
function compares the discriminator’s predicted probabilities (for all K domains) and the ground
truth label (a one-hot vector which specifies the actual domain the data point belongs to).</p>

<p>The QA model is trained with a combined loss function comprised of a standard cross-entropy
loss (CE) plus a domain-invariance term (KLD) that measures the Kullback-Leibler divergence
between the uniform distribution over all K domains and the discriminator’s actual domain prediction. Intuitively, if the QA model is able to learn domain-invariant features that can fool the discriminator, the KLD will be low as the discriminator cannot do better than random guesses in the domain prediction task.</p>

<p>The final loss for the QA model is given by CE + λ * KLD where λ is a hyper-parameter for controlling the importance of adversarial loss. We use λ = 0.01 as previous work finds this value of lambda performs best in ablation studies</p>

<h2 id="using-out-of-domain-data-in-training-and-fine-tuning">
<a class="anchor" href="#using-out-of-domain-data-in-training-and-fine-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using out-of-domain data in training and fine-tuning</h2>
<p>Thus far, the model is trained solely on in-domain data. However, we want to see what happens if we include out-of-domain data in training too. We also want to experiment with doing an additional step of fine-tuning after training where the model exclusively learns from out-of-domain samples. Hopefully, the inclusion of out-of-domain data in training and fine-tuning can improve the model’s ability to generalize to out-of-domain samples.</p>

<h2 id="data-augmentation">
<a class="anchor" href="#data-augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Augmentation</h2>
<p>As we have limited out-of-domain data to train and finetune on, we hypothesize that out-of-domain
data augmentation may help improve the performance of our model. Thus, we expand our out-of-domain samples using 2 methods:</p>

<p><strong>EDA: Synonym Swapping</strong>. We implement the synonym swap method from the <a href="https://github.com/makcedward/nlpaug">nlpaug</a>
package for easy data augmentation. To accomplish this, we replace random words in the context paragraph with its synonyms. Here is an example:</p>

<p>Original context paragraph: “Quentin is a big <strong>fan</strong> of machine learning. He can’t stop <strong>building</strong> models.”</p>

<p>Question: What does Quentin like?</p>

<p>Context paragraph variant 1: “Quentin is a big <strong>lover</strong> of machine learning. He can’t stop <strong>developing</strong> models.”</p>

<p>Context paragraph variant 2: “Quentin is a big <strong>enthusiast</strong> of machine learning. He can’t stop <strong>formulating</strong> models.”</p>

<p>Using this approach, we generate 381 extra context-question pairs from the out-of-domain data.</p>

<p><strong>Synthetic Question Answer Generation.</strong> The first approach creates new variants of <em>context paragraphs</em>. However, we might also want to create new variants of <em>questions</em>. To do so, we leverage <a href="https://arxiv.org/abs/1906.05416">Google’s multitask T5 model</a> (fine-tuned on a SQuAD dataset) to generate synthetic question-answer pairs for a given context paragraph. To ensure <strong>roundtrip consistency</strong>, we take the generated context-question pair and feed it back into the QA portion of the T5 model. If the T5 model is able to predict the correct answer, we keep the synthetic sample. If not, we discard it. Using this approach, we
generate 1579 extra context-question-answer pairs.</p>

<h2 id="domain-alignment">
<a class="anchor" href="#domain-alignment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Domain Alignment</h2>
<p>We also experiment with redefining domains. Typically, in adversarial learning, each dataset is treated as a distinct domain. However, this poses 3 issues:</p>

<ol>
  <li>
    <p>The domain boundaries are not well-defined: SQuAD and Natural Questions are both Wikipedia-based
datasets, so the discriminator is trained to differentiate between relatively similar domains.</p>
  </li>
  <li>
    <p>If we include out-of-domain training data, the number of domains that need to be identified increase from 3
to 6, impeding the discriminator’s ability to effectively differentiate between domains, particularly
when it has few samples to learn from in some domains.</p>
  </li>
  <li>
    <p>If we include out-of-domain training
data, the discriminator faces major class imbalance as there are more than 3500 times more in-domain
samples than out-of-domain samples.</p>
  </li>
</ol>

<p>These challenges may make it difficult for the
discriminator to learn to distinguish between domains. Consequently, the discriminator exerts less pressure on the QA model, diminishing the QA model’s ability to generalize to out-of-domain samples.</p>

<p>To rectify these issues, we introduce Wiki alignment. In Wiki alignment, the Wiki datasets (SQuAD, NaturalQuestions, RelationExtraction) are treated as one domain, while the non-Wiki datasets (NewsQA, DuoRC, RACE) treated as a separate domain. This allows us to partition the sample space into fewer, better-balanced
domains with well-defined boundaries.</p>

<h2 id="tuning-discriminator-architecture">
<a class="anchor" href="#tuning-discriminator-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tuning Discriminator Architecture</h2>
<p>To further improve discriminator learning, we employ two techniques.</p>

<p>Firstly, we incorporate discriminator lambda annealing. The discriminator lambda starts at 0 and is
gradually increased using a tanh function before plateauing at 0.01 at step 20,000. This prevents the
discriminator from initially being overwhelmed with difficult examples, and allows it to progressively
train on harder examples.</p>

<p>Secondly, we incorporate <a href="https://proceedings.mlr.press/v70/arjovsky17a.html">Wasserstein regularization</a> where the weights of the discriminator are
clipped between -0.01 and 0.01 before backward propagation. Weight clipping can enforce the
Lipschitz constraint, which regularizes adversarial training and improves stability.</p>

<h2 id="ensemble-methods">
<a class="anchor" href="#ensemble-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ensemble Methods</h2>
<p>Finally, we explore ensembling different performant models together to reduce overall variance. Intuitively, different models have different noise patterns. By ensembling them together, noise patterns cancel out and the resultant ensemble achieves better and more stable performance. We experiment ensembling the best model for each out-of-domain dataset (total of 3 component models).</p>

<h1 id="overall-results--insights">
<a class="anchor" href="#overall-results--insights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overall Results &amp; Insights</h1>
<p>After running a series of experiments, we find that training a Wiki-aligned adversarial model on additional synthetic out-of-domain samples and subsequently fine-tuning it on augmented oo-domain samples (using synonym replacement) produces the best results. The model achieves an F1 score of <strong>55.53</strong>, which is a <strong>11% improvement in dev F1 over the baseline.</strong></p>

<p>Here are some of our key insights (for a more comprehensive and detailed list, please see the actual report):</p>

<ol>
  <li>
    <p>Without fine-tuning, we find that the adversarial model underperforms the baseline model. In contrast, after fine-tuning, the adversarial model sees a large jump in F1 score and outperforms the baseline. We hypothesize that the model learns domain-invariant features during training but is only able to adapt them to out-of-domain samples after fine-tuning with out-of-domain data. <strong>This suggests that fine-tuning is crucial in “unlocking” the potential of adversarial learning.</strong></p>
  </li>
  <li>
    <p>Wiki alignment is crucial in helping the discriminator learn better, which in turn improves QA model performance. <strong>Having well-defined domains is thus imperative for effective adversarial learning.</strong></p>
  </li>
  <li>
    <p>Including synthetic out-of-domain samples helps training, but hurts fine-tuning. The opposite is true for augmented (synonym replacement) out-of-domain samples. We suspect that the model is very sensitive to the quality of out-of-domain samples during fine-tuning as it is trying to extract precise features from the samples. <strong>As such, synthetic question generation performs more poorly than simple synonym replacement as it attempts to recreate questions from scratch and is thus noisier.</strong>
In contrast, during training, the noise of out-of-domain samples is less of a concern since (i) we are trying to learn general domain-invariant features rather than precise domain-specific features and (ii) the noise is averaged over a much larger dataset. What is important is having a diversity of question-answer pairs for more parts of the context paragraph. Intuitively, the original dataset has large context paragraphs with only a few questions that look at specific parts of the paragraph. Example:</p>
  </li>
</ol>

<p>Context paragraph: “Hi, I am Quentin. I love eating burgers and cooking them by myself. I aspire to be a chef one day and open my own burger restaurant. By the way, my birthday is next month so you know what to get me!”</p>

<p>Question: “When is Quentin’s birthday?”</p>

<p>Answer: “next month”</p>

<p>The model can get away with reading only a small section of the paragraph, which prevents it from fully understanding the context paragraph. If we can create new question-answer pairs that covers the <em>entire</em> context paragraph, we will force our QA model and discriminator to learn about the structure and characteristics of the entire paragraph. This in turn allows it to learn domain-invariant features better. <strong>Synthetic question generation tackles this root problem and thus performs much better than simple synonym replacement when used in training.</strong></p>

<ol>
  <li>Tuning discriminator architecture using Lambda annealing and Wasserstein regularization leads to a slight degradation in performance on our best model. We think that synthetic out-of-domain training data already sufficiently improves discriminator training by providing enough out-of-domain samples from the adversarial model to learn on. <strong>As such, the imposition of additional constraints may be unnecessary and potentially harmful to model performance.</strong> For instance, it is not necessary to anneal lambda from 0 if the discriminator is already able to handle difficult examples from the get-go. Doing so will only deprive it of valuable training time.</li>
</ol>

<p>Finally, we explore ensembling and ensemble together the best models for each out-of-domain dataset. The ensemble achieves a <strong>dev F1 of 57.8</strong>6, which is a** 16% improvement over the baseline.** It also achieves a <strong>test F1 of 65.27</strong>, which is a <strong>10% improvement over baseline.</strong> By averaging across models with different noise patterns, <strong>ensembling is an effective way of boosting performance.</strong></p>

<h1 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1>
<p>We implemented a variety of techniques that boosted the robustness of a QA model to domain shifts, achieving a 16% improvement in dev F1 and a 10% improvement in test F1. Here are some questions that we want to consider exploring:</p>

<ul>
  <li>
    <p>What if we redefine domains in a more computational way (vs using a simple heuristic)? For instance, clustering together samples with similar word embeddings as one domain. This might allow us to pick up deeper domain relationships.</p>
  </li>
  <li>
    <p>What if we explore different ways of augmented out-of-domain samples? We have tried out synonym replacement but what about other techniques such as random insertion (where words are randomly inserted into the paragraph)?</p>
  </li>
</ul>

<p>There remains much work to be done to make our QA systems robust and accessible for all. Thank you for reading!</p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="nicholaslui97/writing"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/writing/projects/nlp/deep%20learning/2022/03/18/Generating-Robustness-Adapting-QA-systems-to-new-domains.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/writing/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/writing/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/writing/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://www.linkedin.com/in/nicholas-lui" target="_blank" title="nicholas-lui"><svg class="svg-icon grey"><use xlink:href="/writing/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
