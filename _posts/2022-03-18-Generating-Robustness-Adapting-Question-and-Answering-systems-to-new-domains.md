---
title: Generating Robustness - Adapting Question and Answering systems to new domains
description: A project that my team worked on for our Stanford CS224N project, which won the Best Poster Award. We implement a variety of techniques (e.g. adversarial learning) that boost the robustness of a question and answering model, improving its ability to generalize to new domains.
toc: true
comments: true
layout: post
image: images/siri.jpg
categories: [projects, nlp, deep learning]
author: Nicholas
---

I would like to express my deep gratitude to my teammates (Helen Gu and Quentin Hsu) for their collaborative spirit, research ambition, and willingness to tackle open-ended problems
with tenacity. Here is our
<a href="https://drive.google.com/file/d/1-cleNk6Auyrk2rEEW7fBM30FPiLhYORX/view?usp=sharing">project report</a>
and <a href="https://drive.google.com/file/d/1qyAD_KEot7g21jRoFcN6Val0RHBb_1Z1/view?usp=sharing">poster</a>. Our project won the <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/project.html">Best Poster Award</a>.

# Motivation
Question and Answering (QA) systems are systems that can automatically answer human questions in a natural language.
They are ubiquitous in everyday life. From the Siri voice assistant on your iPhone to intelligent chatbots, QA systems provide us with
greater convenience, allowing us to access information in an intuitive and personal manner.

The big issue is that QA systems are not robust to domain shifts, diminishing its ability to generalize to domains that it has not been trained on. Let's say that you are a government agency designing an intelligent chatbot that can answer citizens'
questions on government schemes. Citizens ask a question ("what is the age requirement for the new housing subsidy?") and the QA system extracts the answer ("21") from a corpus of information
about the policy (the "context"). The issue is that information about different policies are structured in different ways. The QA system may have been trained on context-question
pairs on housing policy and thus has a good grasp of how information on housing policy is structured, enabling it to perform efficient extractions. However, when it is faced with
context-question pairs from a new policy area (say, financial assistance schemes), it performs poorly as it has little information on the structure and characteristics of information
in that area.

In an ideal world, we would be able to plug this gap by simply getting more labelled data from the new domain and finetuning our QA model on it. However, labelled QA data
is hard to come by and extremely expensive to create (think about the number of man hours needed to create new context-question pairs). As such, we need to explore new techniques
to build robustness in QA systems, allowing them to generalize to unseen domains.

In our project, we implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a **16% increase in F1 score in development** and **10% increase in test**.

# Setup
We are given 3 in-domain training datasets, each with 50,000 samples. The datasets are SQuAD (Wikipedia articles), NaturalQuestions (Wikipedia articles), and NewsQA (news articles). We are also given 3 out-of-domain training datasets, each with approximately 100 samples. The datasets are RelationExtraction (Wikipedia articles), DuoRC (movie reviews), and RACE (examination questions). Performance is evaluated on the out-of-domain validation set.

# Baseline
Our baseline QA model is a DistilBERT trained solely on in-domain data. The QA model looks at a context paragraph and question. It then predicts the start and end position of the answer in the context paragraph. The predicted start and end positions are subsequently compared with the ground truth labels. Our baseline model achieves an **F1 score of 49.88**. In the sections below, I will talk about the techniques we employed, followed by overall results and insights.

# Techniques

## Adversarial Learning
The core technique in our paper is domain adversarial learning, which we adapt from <a href="https://aclanthology.org/D19-5826/">Lee et al (2019)</a>. The domain adversarial learning model is comprised of two components - the QA model (our DistilBERT) and a domain discriminator. During training, the discriminator is trained to predict the domain (dataset) of the
hidden representation produced by the QA model. In contrast, the QA model is penalized for the
success of the discriminator, thus forcing the QA model to learn domain-invariant features such that
it produces a hidden representation that is indistinguishable to the domain discriminator. At the same
time, the domain discriminator also needs to learn what domain-invariant features to keep in order to
maintain its performance on the samples generated by the QA model.

A good analogy is the Angry Birds game! The domain discriminator is the catapult, while the QA model is the tower. The domain discriminator tries to break down the QA tower (guess what domain the QA hidden representation comes from), and the QA model tries to improve its defence against the attacks of the discriminator catapult (learn domain-invariant features to produce hidden representations that are indistinguishable to the discriminator). The end result is a reinforced QA model tower that is adept at learning domain-invariant features.

<img width="60%" alt="space" src="https://user-images.githubusercontent.com/40440105/159158048-0bf596c7-1dbd-463d-8d2b-a8acb89659b9.png">
<center><em>Infographic from our poster</em></center>

The discriminator is trained with a cross-entropy loss function. For a given training point, the loss
function compares the discriminator’s predicted probabilities (for all K domains) and the ground
truth label (a one-hot vector which specifies the actual domain the data point belongs to).

The QA model is trained with a combined loss function comprised of a standard cross-entropy
loss (CE) plus a domain-invariance term (KLD) that measures the Kullback-Leibler divergence
between the uniform distribution over all K domains and the discriminator’s actual domain prediction. Intuitively, if the QA model is able to learn domain-invariant features that can fool the discriminator, the KLD will be low as the discriminator cannot do better than random guesses in the domain prediction task.

The final loss for the QA model is given by CE + λ * KLD where λ is a hyper-parameter for controlling the importance of adversarial loss. We use λ = 0.01 as previous work finds this value of lambda performs best in ablation studies

## Using out-of-domain data in training and fine-tuning
Thus far, the model is trained solely on in-domain data. However, we want to see what happens if we include out-of-domain data in training too. We also want to experiment with an additional step of fine-tuning after training where the model exclusively learns from out-of-domain samples. Hopefully, the inclusion of out-of-domain data in training and fine-tuning can improve the model's ability to generalize to out-of-domain samples. 

## Data Augmentation
As we have limited out-of-domain data to train and finetune on, we hypothesize that out-of-domain
data augmentation may help improve the performance of our model. Thus, we expand our out-of-domain samples using 2 methods:

**Easy Data Augmentation (EDA): Synonym Swapping**. We implement the synonym swap method from the <a href="https://github.com/makcedward/nlpaug">nlpaug</a>
package for easy data augmentation. To accomplish this, we replace random words in the context paragraph with its synonyms. Here is an example:

Original context paragraph: "Quentin is a big **fan** of machine learning. He can't stop **building** models."

Question: What does Quentin like?

Context paragraph variant 1: "Quentin is a big **lover** of machine learning. He can't stop **developing** models."

Context paragraph variant 2: "Quentin is a big **enthusiast** of machine learning. He can't stop **formulating** models."

Using this approach, we generate 381 extra context-question pairs from the out-of-domain data.

**Synthetic Question Answer Generation.** The first approach creates new variants of _context paragraphs_. However, we might also want to create new variants of _questions_. To do so, we leverage <a href="https://arxiv.org/abs/1906.05416">Google's multitask T5 model</a> (fine-tuned on a SQuAD dataset) to generate synthetic question-answer pairs for a given context paragraph. To ensure **roundtrip consistency**, we take the generated context-question pair and feed it back into the QA portion of the T5 model. If the T5 model is able to predict the correct answer, we keep the synthetic sample. If not, we discard it. Using this approach, we
generate 1579 extra context-question-answer pairs.

## Domain Alignment
We also experiment with redefining domains. Typically, in adversarial learning, each dataset is treated as a distinct domain. However, this poses 3 issues:

1. The domain boundaries are not well-defined: SQuAD and Natural Questions are both Wikipedia-based
datasets, so the discriminator is trained to differentiate between relatively similar domains.

2. If we include out-of-domain training data, the number of domains that need to be identified increase from 3
to 6, impeding the discriminator’s ability to effectively differentiate between domains, particularly
when it has few samples to learn from in some domains.

3. If we include out-of-domain training
data, the discriminator faces major class imbalance as there are more than 3500 times more in-domain
samples than out-of-domain samples.

These challenges may make it difficult for the
discriminator to learn to distinguish between domains. Consequently, the discriminator exerts less pressure on the QA model, diminishing the QA model’s ability to generalize to out-of-domain samples.

To rectify these issues, we introduce Wiki alignment. In Wiki alignment, the Wiki datasets (SQuAD, NaturalQuestions, RelationExtraction) are treated as one domain, while the non-Wiki datasets (NewsQA, DuoRC, RACE) treated as a separate domain. This allows us to partition the sample space into fewer, better-balanced
domains with well-defined boundaries.

## Tuning Discriminator Architecture
To further improve discriminator learning, we employ two techniques.

Firstly, we incorporate discriminator lambda annealing. The discriminator lambda starts at 0 and is
gradually increased using a tanh function before plateauing at 0.01 at step 20,000. This prevents the
discriminator from initially being overwhelmed with difficult examples, and allows it to progressively
train on harder examples.

Secondly, we incorporate <a href="https://proceedings.mlr.press/v70/arjovsky17a.html">Wasserstein regularization</a> where the weights of the discriminator are
clipped between -0.01 and 0.01 before backward propagation. Weight clipping can enforce the
Lipschitz constraint, which regularizes adversarial training and improves stability.

## Ensemble Methods
Finally, we explore ensembling different performant models together to reduce overall variance. Intuitively, different models have different noise patterns. By ensembling them together, noise patterns cancel out and the resultant ensemble achieves better and more stable performance. We experiment ensembling the best models for each out-of-domain dataset.

# Overall Results & Insights
After running a series of experiments, we find that training a Wiki-aligned adversarial model on additional synthetic out-of-domain samples and subsequently fine-tuning it on EDA (synonym replacement) out-of-domain samples produces the best results. The model achieves an F1 score of **55.53**, which is a **11% improvement in dev F1 over the baseline.**

Here are some of our key insights (for a more comprehensive and detailed list, please see the actual report):

1.	Without fine-tuning, we find that the adversarial model underperforms the baseline model. In contrast, after fine-tuning, the adversarial model sees a large jump in F1 score and outperforms the baseline. We hypothesize that the model learns domain-invariant features during training but is only able to adapt them to out-of-domain samples after fine-tuning with out-of-domain data. **This suggests that fine-tuning is crucial in “unlocking” the potential of adversarial learning.**

2.	Wiki alignment is crucial in helping the discriminator learn better, which in turn improves QA model performance. **Having well-defined domains is thus imperative for effective adversarial learning.**

3.	Including synthetic out-of-domain samples helps training, but hurts fine-tuning. The opposite is true for EDA (synonym replacement) out-of-domain samples. We suspect that the model is very sensitive to the quality of out-of-domain samples during fine-tuning as it is trying to extract precise features from the samples. **As such, synthetic question generation performs more poorly than simple synonym replacement as it attempts to recreate questions from scratch and is thus noisier.**

In contrast, during training, the noise of out-of-domain samples is less of a concern since (i) we are trying to learn general domain-invariant features rather than precise domain-specific features and (ii) the noise is averaged over a much larger dataset. What is important is having a diversity of question-answer pairs for more parts of the context paragraph. Intuitively, the original dataset contains context paragraphs with only a few question-answer pairs that look at small sections of the paragarph. Example:

Context paragraph: “Hi, I am Quentin. I love eating burgers and cooking them by myself. I aspire to be a chef one day and open my own burger restaurant. By the way, my birthday is next month so you know what to get me!”

Question: “When is Quentin’s birthday?”

Answer: “next month”

Thus, the model can get away with reading only a small section of the paragraph, preventing it from fully understanding the context paragraph. If we can create new question-answer pairs that covers the _entire_ context paragraph, we will force our QA model and discriminator to learn about the structure and characteristics of the entire paragraph. This in turn allows it to learn domain-invariant features better. **By increasing the diversity of question-answer pairs, synthetic question generation tackles this root problem and thus performs much better than simple synonym replacement during training.**

4.	Tuning discriminator architecture using Lambda annealing and Wasserstein regularization leads to a slight degradation in performance on our best model. We think that synthetic out-of-domain training data already sufficiently improves discriminator training by providing enough out-of-domain samples from the adversarial model to learn on. **As such, the imposition of additional constraints may be unnecessary and potentially harmful to model performance.** For instance, it is not necessary to anneal lambda from 0 if the discriminator is already able to handle difficult examples from the get-go. Doing so will only deprive it of valuable training time.

Finally, we explore ensembling and ensemble together the best models for each out-of-domain dataset. The ensemble achieves a **dev F1 of 57.86**, which is a **16% improvement over the baseline.** It also achieves a **test F1 of 65.27**, which is a **10% improvement over baseline.** By averaging across models with different noise patterns, **ensembling is an effective way of boosting performance.**

# Conclusion
We implemented a variety of techniques that boosted the robustness of a QA model to domain shifts, achieving a 16% improvement in dev F1 and a 10% improvement in test F1. Here are some questions that we want to continue exploring:

-	What if we redefine domains in a more computational way (vs using a simple heuristic)? For instance, clustering together samples with similar word embeddings as one domain. This might allow us to pick up deeper and more nuanced domain relationships.

-	What if we explore different ways of augmenting out-of-domain samples? We have tried out synonym replacement but what about other techniques such as random insertion (where words are randomly inserted into the paragraph) and random deletion (where words are randomly deleted from the paragarph)?

There remains much work to be done to make our QA systems robust and accessible for all. Thank you for reading!

_(Cover picture from TechRepublic)_





