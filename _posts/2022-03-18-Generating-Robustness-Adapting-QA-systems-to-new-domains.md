---
title: Generating Robustness - Adapting QA systems to new domains
description: A project that my team worked on for our Stanford CS224N project. We implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a 16% increase in F1 score in development and 10% increase in test.
toc: true
comments: true
layout: post
categories: [projects, nlp, deep learning]
author: Nicholas
---

I would like to express my deep gratitude to my teammates (Helen Gu and Quentin Hsu) for their collaborative spirit, research ambition, and willingness to tackle open-ended problems
with tenacity. Here is our
<a href="https://drive.google.com/file/d/1-cleNk6Auyrk2rEEW7fBM30FPiLhYORX/view?usp=sharing">project report</a>
and <a href="https://drive.google.com/file/d/1qyAD_KEot7g21jRoFcN6Val0RHBb_1Z1/view?usp=sharing">poster</a>.

# Motivation
Question and Answering (QA) systems are systems that can automatically answer human questions in a natural language.
They are ubiquitous in everyday life. From the Siri voice assistant on your iPhone to intelligent chatbots, QA systems provide us with
greater convenience, allowing us to access information in an intuitive and personal manner.

The big issue is that QA systems are not robust to domain shifts, diminishing its ability to generalize to domains that it has not been trained on. Let's say that you are a government agency designing an intelligent chatbot that can answer citizens'
questions on government schemes. Citizens ask a question ("what is the age requirement for the new housing subsidy?") and the QA system extracts the answer ("21") from a corpus of information
about the policy (the "context"). The issue is that information about different policies are structured in different ways. The QA system may have been trained on context-question
pairs on housing policy and thus has a good grasp of how information on housing policy is structured, enabling it to perform efficient extractions. However, when it is faced with
context-question pairs from a new policy area (say, financial assistance schemes), it performs poorly as it has little information on the structure and characteristics of information
in that area.

In an ideal world, we would be able to plug this gap by simply getting more labelled data from the new domain and finetuning our QA model on it. However, labelled QA data
is hard to come by and extremely expensive to create (think about the number of man hours needed to create new context-question pairs). As such, we need to explore new techniques
to build robustness in QA systems, allowing them to generalize to unseen domains.

In our project, we implement a variety of techniques that boost the robustness of a QA model trained with domain adversarial learning and evaluated on out-of-domain data, yielding a **16% increase in F1 score in development** and **10% increase in test**. We find that the following innovations boost model performance: 1) finetuning the model on augmented out-of-domain data, 2) redefining domains during adversarial training to simplify the domain discriminator’s task, and 3) supplementing the training data with synthetic QA pairs generated with roundtrip consistency. We also ensemble the best-performing models on each dataset and find that ensembling yields further performance increases.

# Setup
We are given 3 in-domain training datasets, each with 50,000 samples. The datasets are SQuAD (Wikipedia articles), NaturalQuestions (Wikipedia articles), and NewsQA (news articles). We are also given 3 out-of-domain training datasets, each with approximately 100 samples. The datasets are RelationExtraction (Wikipedia articles), DuoRC (movie reviews), and RACE (examination questions). Performance is evaluated on the out-of-domain validation set.

# Baseline
Our baseline model is a DistilBERT trained solely on in-domain data. It achieves an F1 score of 49.88. In the sections below, I will talk about the techniques we employed, followed by any interesting insights we gleaned.

# Techniques

## Adversarial Learning
The core technique in our paper is domain adversarial learning, which we adapt from <a href="https://aclanthology.org/D19-5826/">Lee et al (2019)</a>. The domain adversarial learning model is comprised of two components - the QA model (our DistilBERT) and a domain discriminator. During training, the discriminator is trained to predict the domain (dataset) of the
hidden representation produced by the QA model. In contrast, the QA model is penalized for the
success of the discriminator, thus forcing the QA model to learn domain-invariant features such that
it produces a hidden representation that is indistinguishable to the domain discriminator. At the same
time, the domain discriminator also needs to learn what domain-invariant features to keep in order to
maintain its performance on the samples generated by the QA model.

A good analogy is the Angry Birds game! The domain discriminator is the catapult, while the QA model is the tower. The domain discriminator tries to break down the QA tower (guess what domain the QA hidden representation comes from), and the QA model tries to improve its defence against the attacks of the discriminator catapult (learn domain-invariant features to produce hidden representations that are indistinguishable to the discriminator). The end result is a reinforced QA model tower that is adept at learning domain-invariant features.

<img width="60%" alt="space" src="https://user-images.githubusercontent.com/40440105/159158048-0bf596c7-1dbd-463d-8d2b-a8acb89659b9.png">
<center><em>Infographic from our poster</em></center>

The discriminator is trained with a cross-entropy loss function. For a given training point, the loss
function compares the discriminator’s predicted probabilities (for all K domains) and the ground
truth label (a one-hot vector which specifies the actual domain the data point belongs to).

The QA model is trained with a combined loss function comprised of a standard cross-entropy
loss (CE) plus a domain-invariance term (KLD) that measures the Kullback-Leibler divergence
between the uniform distribution over all K domains and the discriminator’s actual domain prediction. Intuitively, if the QA model is able to learn domain-invariant features that can fool the discriminator, the KLD will be low as the discriminator cannot do better than random guesses in the domain prediction task.

The final loss for the QA model is given by CE + λ * KLD where λ is a hyper-parameter for controlling the importance of adversarial loss. We use λ = 0.01 as previous work finds this value of lambda performs best in ablation studies

## Using out-of-domain data in training and fine-tuning
Thus far, the model is trained solely on in-domain data. However, we want to see what happens if we include out-of-domain data in training too. We also want to experiment with doing an additional step of fine-tuning after training where the model exclusively learns from out-of-domain samples. Hopefully, the inclusion of out-of-domain data in training and fine-tuning can improve the model's ability to generalize to out-of-domain samples. 

## Data Augmentation
As we have limited out-of-domain data to train and finetune on, we hypothesize that out-of-domain
data augmentation may help improve the performance of our model. Therefore, we implement two
techniques to expand our out-of-domain data samples.

**EDA: Synonym Swapping**. We implement the synonym swap method from the <a href="https://github.com/makcedward/nlpaug">nlpaug</a>
package for easy data augmentation. To accomplish this, we replace random words in the context paragraph with its synonyms. Here is an example:

Original context paragraph: "Quentin is a big **fan** of machine learning. He can't stop **building** models."

Question: What does Quentin like?

Context paragraph variant 1: "Quentin is a big **lover** of machine learning. He can't stop **developing** models."

Context paragraph variant 2: "Quentin is a big **enthusiast** of machine learning. He can't stop **formulating** models."

Using this approach, we generate 381 extra context-question pairs from the out-of-domain data.

**Synthetic Question Answer Generation.** The first approach creates new variants of _context paragraphs_. However, we might also want to create new variants of _questions_. To do so, we leverage <a href="https://arxiv.org/abs/1906.05416">Google's multitask T5 model</a> (fine-tuned on a SQuAD dataset) to generate synthetic question-answer pairs for a given context paragraph. To ensure **roundtrip consistency**, we remove duplicate questions and then rerun
the QA portion of the T5 model to repredict the answer given the generated question and context
chunk. We only keep question answer pairs in which the QA model predicts the same answer as the
generated answer to ensure we have high-quality question-answer pairs. Using this approach, we
generate 1579 extra context-question-answer pairs.

## Domain Alignment
We also experiment with redefining domains. Typically, in adversarial learning, each dataset is treated as a distinct domain. However, this poses 3 issues:

1. The domain boundaries are not well-defined: SQuAD and Natural Questions are both Wikipedia-based
datasets, so the discriminator is trained to differentiate between relatively similar domains.

2. If we include out-of-domain training data, the number of domains that need to be identified increase from 3
to 6, impeding the discriminator’s ability to effectively differentiate between domains, particularly
when it has few samples to learn from in some domains.

3. If we include out-of-domain training
data, the discriminator faces major class imbalance as there are more than 3500 times more in-domain
samples than out-of-domain samples.

We hypothesize that these challenges make it difficult for the
discriminator to learn to distinguish between domains. Consequently, the discriminator exerts less pressure on the QA model, diminishing the QA model’s ability to generalize to out-of-domain samples.

To rectify these issues, we introduce Wiki alignment. In Wiki alignment, the Wiki datasets (SQuAD, NaturalQuestions, RelationExtraction) are treated as one domain, while the non-Wiki datasets (NewsQA, DuoRC, RACE) treated as a separate domain. This allows us to partition the sample space into fewer, better-balanced
domains with well-defined boundaries.

## Tuning Discriminator Architecture
To further improve discriminator learning, we employ two techniques.

Firstly, we incorporate discriminator lambda annealing. The discriminator lambda starts at 0 and is
gradually increased using a tanh function before plateauing at 0.01 at step 20,000. This prevents the
discriminator from initially being overwhelmed with difficult examples, and allows it to progressively
train on harder examples.

Secondly, we incorporate Wasserstein regularization where the weights of the discriminator are
clipped between -0.01 and 0.01 before backward propagation. Weight clipping can enforce the
Lipschitz constraint, which regularizes adversarial training and improves stability.

## Ensemble Methods
Finally, we explore ensembling different performant models together to reduce overall variance. Intuitively, different models have different noise patterns. By ensembling them together, noise patterns cancel out and the resultant ensemble achieves better and more stable performance. We experiment ensembling the best model for each out-of-domain dataset (total of 3 component models).

# Overall Results & Insights




